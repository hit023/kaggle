{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column-1 contains the labels, so cast it to int\n",
    "#other columns are pixel intensities, so cast them to floating-point numbers\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "train = df.as_matrix()\n",
    "\n",
    "#intensities\n",
    "train_y = train[:,0].astype('int8')\n",
    "train_x = train[:,1:].astype('float64')\n",
    "\n",
    "train = None\n",
    "\n",
    "test = pd.read_csv(\"test.csv\").as_matrix().astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4132\n",
      "4684\n",
      "4177\n",
      "4351\n",
      "4072\n",
      "3795\n",
      "4137\n",
      "4401\n",
      "4063\n",
      "4188\n"
     ]
    }
   ],
   "source": [
    "# note that we will have bad accuracy if the examples for each digit are not uniformly distributed.\n",
    "#So check if they are.\n",
    "no_of_train = train_y.shape[0]\n",
    "counts = [0]*10\n",
    "for i in range(no_of_train):\n",
    "    counts[train_y[i]] += 1\n",
    "for i in counts:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost Uniform. So apply NN happily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise to keep floating-point underflow in check!\n",
    "test /= 255\n",
    "train_x /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at this : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n",
    "#gives you one-hot encoding.\n",
    "train_y = pd.get_dummies(train_y).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one-hot encoding is used here and we have to predict from 10 labels ie: the most 'probable', makes sense to use Softmax function : \n",
    "\\begin{equation*}\n",
    "y=σ(x)=\\frac{e^x}{\\sum_{i=1}^Ne^x}\n",
    "\\end{equation*}\n",
    "\n",
    "Recall that the derivative of sigmoid is :\n",
    "\\begin{equation*}\n",
    "y=σ(x)=y*(1-y)\n",
    "\\end{equation*}\n",
    "I intend to apply the most-widely used combination : **Cross-entropy as Loss-function + Softmax classification + weight-decay**. (Might apply RBMs in another notebook on MNIST, to learn the same!). Read this : http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n",
    "\n",
    "No. of hidden layers : 2 with 500 and 300 units in each respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialised using : random gaussian function with mean=zero and variance between 1/sqtr(num_inputs_layer)\n",
    "def getWeights():\n",
    "    inp = 784\n",
    "    w1_dim = 500\n",
    "    w2_dim = 300\n",
    "    classes = 10\n",
    "    \n",
    "    #layer1\n",
    "    w1 = np.random.normal(0,inp**-0.5,[inp,w1_dim])\n",
    "    b1 = np.random.normal(0,inp**-0.5,[1,w1_dim])\n",
    "    \n",
    "    #layer2\n",
    "    w2 = np.random.normal(0,w1_dim**-0.5,[w1_dim,w2_dim])\n",
    "    b2 = np.random.normal(0,w1_dim**-0.5,[1,w2_dim])\n",
    "    \n",
    "    #layer3\n",
    "    w3 = np.random.normal(0,w2_dim**-0.5,[w2_dim,classes])\n",
    "    b3 = np.random.normal(0,w2_dim**-0.5,[1,classes])\n",
    "    \n",
    "    return [w1,b1,w2,b2,w3,b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this : http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf to learn more about 'Dropout'. It is mainly to avoid overfitting by ignoring a few input units. Here, 'p' is the probability that a unit will be ignored. So simply zero out the selected ones. And n testing, the entire network is considered and each activation is reduced by a factor p. https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 : see this for a condensed version of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=1)).T\n",
    "\n",
    "def ReLu(x,der=False):\n",
    "    if der==False:\n",
    "        return x*(x>0)\n",
    "    else:\n",
    "        return 1*(x>0)\n",
    "\n",
    "def dropout(x,p):\n",
    "    m = np.random.binomial([np.ones_like(x)],(1-p))[0] / (1-p)\n",
    "    #note that only the first dimension is being taken.\n",
    "    return x*m\n",
    "                           \n",
    "def forward_prop(x,weights,p=0):\n",
    "    w1,b1,w2,b2,w3,b3 = weights                    \n",
    "\n",
    "    #in case you wondering : https://stackoverflow.com/questions/27385633/what-is-the-symbol-for-in-python\n",
    "    z1 = ReLu(x@w1 + b1)\n",
    "    z1 = dropout(z1,p)\n",
    "                           \n",
    "    z2 = ReLu(z1@w2 + b2)\n",
    "    z2 = dropout(z2,p)\n",
    "                           \n",
    "    return [z1,z2,softmax(z2@w3 + b3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cross-entropy loss function when dealing with classification:\n",
    "\\begin{equation*}\n",
    "-\\frac{1}{N}.{\\sum_{i=1}^N[y*log(a)+(1-y)log(1-a)]}\n",
    "\\end{equation*}\n",
    "\n",
    "Here is a refresher(I knew you'd forget!): https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    if x!=0:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return -np.inf #don't worry, taken care by nan_to_num : \n",
    "    #https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.nan_to_num.html\n",
    "    \n",
    "def log(y):\n",
    "    return [[log2(nx) for nx in x] for x in y]\n",
    "\n",
    "def cost(a,y):\n",
    "    loss = -np.mean((np.nan_to_num(y*log(a)) + np.nan_to_num((1-y)*log(1-a))),keepdims = True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 784)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add cross-validation\n",
    "frac = 0.2\n",
    "sz = round(train_x.shape[0]*frac)\n",
    "\n",
    "indices = np.arange(no_of_train)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_x[indices[sz:]]\n",
    "x_valid = train_x[indices[:sz]]\n",
    "\n",
    "y_train = train_y[indices[sz:]]\n",
    "y_valid = train_y[indices[:sz]]\n",
    "\n",
    "train_x = None\n",
    "train_y = None\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it comes : Back Prop!! Just apply chain-rule repeatedly to arrive at those complex equations that we got in Stanford ML course! \n",
    "For output layer : \n",
    "\\begin{equation*}\n",
    "\\nabla W_j=\\frac{dL(W_{ij})}{dW_{ij}} = \\frac{1}{N}.(t-y)X_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "For hidden layer :\n",
    "\\begin{equation*}\n",
    "\\nabla W_j=\\frac{dL(W_{ij})}{dW_{ij}} =  \\frac{1}{N}.(t-y).W_{ij}.\\sigma(x)^{'}X_{ij}\n",
    "\\end{equation*}\n",
    "Enter Momentum! Used to avoid getting trapped in local minima by taking leaps and decreases convergence time. Read this : https://www.quora.com/What-does-momentum-mean-in-neural-networks\n",
    "\\begin{equation*}\n",
    "V_{i+1} = \\gamma V_{i} + \\eta.\\nabla L(W) \\\\\n",
    "W = W - V_{i+1}\n",
    "\\end{equation*}\n",
    "Also, add regularisation using L2 norm.\n",
    "\\begin{equation*}\n",
    "L = -\\frac{1}{N}.{\\sum_{i=1}^N[ylog(a)+(1-y)log(1-a)]} + \\frac{1}{2}.\\lambda.\\sum_{j=1}^{nj}\\sum_{i=1}^{ni}W_{ij}^{2}\n",
    "\\end{equation*}\n",
    "As usual, you can't not look at this : http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(weights,x,y,op,eta_lr,gamma_mom,lbda_reg,cache = None):\n",
    "    w1,b1,w2,b2,w3,b3 = weights\n",
    "    if cache == None:\n",
    "        v_w1 = np.zeros_like(w1)\n",
    "        v_w2 = np.zeros_like(w2)\n",
    "        v_w3 = np.zeros_like(w3)\n",
    "        v_b1 = np.zeros_like(b1)\n",
    "        v_b2 = np.zeros_like(b2)\n",
    "        v_b3 = np.zeros_like(b3)\n",
    "    \n",
    "    else:\n",
    "        v_w1,v_b1,v_w2,v_b2,v_w3,v_b3 = cache\n",
    "    \n",
    "    z1,z2,a = op\n",
    "    w3_delta = (y-a)/x.shape[0]\n",
    "    w2_error = w3_delta@w3.T\n",
    "    w2_delta = w2_error*ReLu(z2,der=True)\n",
    "    w1_error = w2_delta@w2.T\n",
    "    w1_delta = w1_error*ReLu(z1,der=True)\n",
    "    \n",
    "    eta_lr = -eta_lr\n",
    "    #print(w3_delta.shape,w2_delta.shape,w1_delta.shape)\n",
    "    #look at the last link in the previous cell for the clearest explanation for the following expressions.\n",
    "    #incorporate l2 regularisation, too.\n",
    "    v_w3 = gamma_mom*v_w3 + eta_lr*(z2.T@w3_delta + lbda_reg*w3)\n",
    "    v_b3 = gamma_mom*v_b3 + eta_lr*(w3_delta.sum(axis=0) + lbda_reg*b3)\n",
    "    \n",
    "    v_w2 = gamma_mom*v_w2 + eta_lr*(z1.T@w2_delta + lbda_reg*w2)\n",
    "    v_b2 = gamma_mom*v_b2 + eta_lr*(w2_delta.sum(axis=0) + lbda_reg*b2)\n",
    "    \n",
    "    v_w1 = gamma_mom*v_w1 + eta_lr*(x.T@w1_delta + lbda_reg*w1)\n",
    "    v_b1 = gamma_mom*v_b1 + eta_lr*(w1_delta.sum(axis=0) + lbda_reg*b1)\n",
    "\n",
    "    w3 -= v_w3\n",
    "    b3 -= v_b3\n",
    "    \n",
    "    w2 -= v_w2\n",
    "    b2 -= v_b2\n",
    "    \n",
    "    w1 -= v_w1\n",
    "    b1 -= v_b1\n",
    "    \n",
    "    weights = [w1,b1,w2,b2,w3,b3]\n",
    "    cache = [v_w1,v_b1,v_w2,v_b2,v_w3,v_b3]\n",
    "    \n",
    "    return weights,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to improve the performance of the neural network. Taken from : https://gist.github.com/fmder/e28813c1e8721830ff9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,y):\n",
    "    tot = 0\n",
    "    n = pred.shape[0]\n",
    "    pred = np.argmax(pred,axis = 1)\n",
    "    y = np.argmax(y,axis = 1)\n",
    "    for i in zip(pred,y):\n",
    "        if i[0] == i[1]:\n",
    "            tot += 1 \n",
    "    return (tot*100)/n\n",
    "    \n",
    "def run(weights,x_train,y_train,x_valid,y_valid,epochs = 10,batch_sz = 25,eta = 1e-3,\n",
    "        decay = 0,momentum=0,l2=0.001,p=0):\n",
    "    cache = None\n",
    "    history = [[],[]] \n",
    "    mtime = 0\n",
    "    index = np.arange(x_train.shape[0])\n",
    "    for j in range(epochs):\n",
    "        t = 0\n",
    "        np.random.shuffle(index)\n",
    "        iterations = round(x_train.shape[0]/batch_sz)\n",
    "        sacc = 0\n",
    "        sloss = 0\n",
    "        print(\"Epoch: {0} / {1}\\n\".format(j+1,epochs))\n",
    "        stime = 0\n",
    "        timeIT = time.time()\n",
    "        for i in range(iterations):\n",
    "            timeI = time.time()\n",
    "            f = i*batch_sz\n",
    "            l = f + batch_sz\n",
    "            #all this to take care when the number of samples is not exaftly divisible by batch_sz\n",
    "            if l>x_train.shape[0]-1 :\n",
    "                l = x_train.shape[0]\n",
    "            x = np.array([elastic_transform(xx.reshape(28,28),15,3).reshape(784) for xx in x_train[index[f:l]]])\n",
    "            y = y_train[index[f:l]]\n",
    "            outputs = forward_prop(x,weights,p)\n",
    "            loss = cost(outputs[-1],y)\n",
    "            accuracy_t = accuracy(outputs[-1],y)\n",
    "            sacc += accuracy_t\n",
    "            sloss += loss\n",
    "            accuracy_train = sacc/(i+1)\n",
    "            loss_train = sloss/(i+1)\n",
    "            #SGD(weights,x,y,op,eta_lr,gamma_mom,lbda_reg,cache = None)\n",
    "            weights,cache = SGD(weights,x,y,outputs,eta,momentum,l2,cache)\n",
    "            t += x.shape[0]\n",
    "            stime += time.time() - timeI\n",
    "            mtime = stime/(i+1)\n",
    "            mTimeT = mtime * (iterations-i-1)\n",
    "            sys.stdout.write(\"\\r%5d/%5d ETA: %3d s - loss : %.4f acc : %.4f\" % (t,x_train\\\n",
    "            .shape[0],mTimeT,loss_train,accuracy_train))\n",
    "            history[0].append([loss_train,accuracy_train])\n",
    "        mtime = time.time()-timeIT\n",
    "        eta = eta - (eta*decay)\n",
    "        \n",
    "        outputs = forward_prop( x_valid,weights)\n",
    "        \n",
    "        loss_valid = cost(outputs[-1], y_valid)\n",
    "        accuracy_valid = accuracy(outputs[-1], y_valid)\n",
    "        sys.stdout.write(\"\\r%5d/%5d ETA: %3d s loss: %.4f  acc: %.4f - lossValid: %.4f  accValid: %.4f \" % ( t, x_train.shape[0],\n",
    "        mtime, loss_train, accuracy_train, loss_valid, accuracy_valid))\n",
    "        history[1].append([loss_valid, accuracy_valid])\n",
    "    return weights,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0954  acc: 81.2113 - lossValid: 0.0278  accValid: 94.9048 Epoch: 2 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0545  acc: 90.1101 - lossValid: 0.0210  accValid: 96.3095 Epoch: 3 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0444  acc: 91.9881 - lossValid: 0.0178  accValid: 96.9405 Epoch: 4 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0382  acc: 93.4107 - lossValid: 0.0150  accValid: 97.4643 Epoch: 5 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0342  acc: 93.8363 - lossValid: 0.0134  accValid: 97.7738 Epoch: 6 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0322  acc: 94.2321 - lossValid: 0.0126  accValid: 97.8929 Epoch: 7 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0304  acc: 94.8214 - lossValid: 0.0114  accValid: 98.1310 Epoch: 8 / 30\n",
      "\n",
      "33600/33600 ETA:  62 s loss: 0.0282  acc: 95.0833 - lossValid: 0.0106  accValid: 98.1310 Epoch: 9 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0275  acc: 94.9792 - lossValid: 0.0102  accValid: 98.2619 Epoch: 10 / 30\n",
      "\n",
      "33600/33600 ETA:  62 s loss: 0.0261  acc: 95.3304 - lossValid: 0.0103  accValid: 98.2381 Epoch: 11 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0239  acc: 95.7054 - lossValid: 0.0094  accValid: 98.4048 Epoch: 12 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0247  acc: 95.7024 - lossValid: 0.0088  accValid: 98.5952 Epoch: 13 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0229  acc: 95.9911 - lossValid: 0.0092  accValid: 98.4643 Epoch: 14 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0217  acc: 96.0774 - lossValid: 0.0088  accValid: 98.5000 Epoch: 15 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0219  acc: 96.1667 - lossValid: 0.0089  accValid: 98.5714 Epoch: 16 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0208  acc: 96.3185 - lossValid: 0.0090  accValid: 98.6548 Epoch: 17 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0214  acc: 96.3036 - lossValid: 0.0083  accValid: 98.6667 Epoch: 18 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0206  acc: 96.4107 - lossValid: 0.0081  accValid: 98.6905 Epoch: 19 / 30\n",
      "\n",
      "33600/33600 ETA:  64 s loss: 0.0200  acc: 96.5089 - lossValid: 0.0085  accValid: 98.7024 Epoch: 20 / 30\n",
      "\n",
      "33600/33600 ETA:  65 s loss: 0.0196  acc: 96.5238 - lossValid: 0.0081  accValid: 98.7500 Epoch: 21 / 30\n",
      "\n",
      "33600/33600 ETA:  67 s loss: 0.0190  acc: 96.6310 - lossValid: 0.0078  accValid: 98.7976 Epoch: 22 / 30\n",
      "\n",
      "33600/33600 ETA:  66 s loss: 0.0194  acc: 96.6548 - lossValid: 0.0078  accValid: 98.6905 Epoch: 23 / 30\n",
      "\n",
      "33600/33600 ETA:  64 s loss: 0.0189  acc: 96.6726 - lossValid: 0.0080  accValid: 98.7262 Epoch: 24 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0184  acc: 96.7560 - lossValid: 0.0076  accValid: 98.7976 Epoch: 25 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0187  acc: 96.6101 - lossValid: 0.0077  accValid: 98.7024 Epoch: 26 / 30\n",
      "\n",
      "33600/33600 ETA:  61 s loss: 0.0182  acc: 96.8095 - lossValid: 0.0076  accValid: 98.7976 Epoch: 27 / 30\n",
      "\n",
      "33600/33600 ETA:  64 s loss: 0.0181  acc: 96.7440 - lossValid: 0.0074  accValid: 98.8333 Epoch: 28 / 30\n",
      "\n",
      "33600/33600 ETA:  66 s loss: 0.0178  acc: 96.9167 - lossValid: 0.0074  accValid: 98.8810 Epoch: 29 / 30\n",
      "\n",
      "33600/33600 ETA:  67 s loss: 0.0173  acc: 96.9554 - lossValid: 0.0073  accValid: 98.8214 Epoch: 30 / 30\n",
      "\n",
      "31700/33600 ETA:   3 s - loss : 0.0173 acc : 97.0379"
     ]
    }
   ],
   "source": [
    "weights = getWeights()\n",
    "\n",
    "alpha = 1e-1\n",
    "epochs = 30\n",
    "nbatchs = 100\n",
    "weights, history = run(weights, \n",
    "              x_train, y_train, \n",
    "              x_valid, y_valid, \n",
    "              epochs = epochs,\n",
    "              batch_sz=nbatchs, \n",
    "              eta = alpha, \n",
    "              decay = 0.1, \n",
    "              momentum = 0.9, \n",
    "              l2 = 1e-7, \n",
    "              p = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
